
@article{cerqueira_evaluating_2020,
	title = {Evaluating time series forecasting models: an empirical study on performance estimation methods},
	volume = {109},
	issn = {1573-0565},
	shorttitle = {Evaluating time series forecasting models},
	url = {https://doi.org/10.1007/s10994-020-05910-7},
	doi = {10.1007/s10994-020-05910-7},
	abstract = {Performance estimation aims at estimating the loss that a predictive model will incur on unseen data. This process is a fundamental stage in any machine learning project. In this paper we study the application of these methods to time series forecasting tasks. For independent and identically distributed data the most common approach is cross-validation. However, the dependency among observations in time series raises some caveats about the most appropriate way to estimate performance in this type of data. Currently, there is no consensual approach. We contribute to the literature by presenting an extensive empirical study which compares different performance estimation methods for time series forecasting tasks. These methods include variants of cross-validation, out-of-sample (holdout), and prequential approaches. Two case studies are analysed: One with 174 real-world time series and another with three synthetic time series. Results show noticeable differences in the performance estimation methods in the two scenarios. In particular, empirical experiments suggest that blocked cross-validation can be applied to stationary time series. However, when the time series are non-stationary, the most accurate estimates are produced by out-of-sample methods, particularly the holdout approach repeated in multiple testing periods.},
	language = {en},
	number = {11},
	urldate = {2022-04-14},
	journal = {Machine Learning},
	author = {Cerqueira, Vitor and Torgo, Luis and Mozetiƒç, Igor},
	month = nov,
	year = {2020},
	keywords = {Cross validation, Forecasting, Model selection, Performance estimation, Time series},
	pages = {1997--2028},
	file = {Full Text PDF:C\:\\Users\\Acer\\Zotero\\storage\\EXF8EHX8\\Cerqueira et al. - 2020 - Evaluating time series forecasting models an empi.pdf:application/pdf},
}

@techreport{schnaubelt_comparison_2019,
	type = {Working {Paper}},
	title = {A comparison of machine learning model validation schemes for non-stationary time series data},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/209136},
	abstract = {Machine learning is increasingly applied to time series data, as it constitutes an attractive alternative to forecasts based on traditional time series models. For independent and identically distributed observations, cross-validation is the prevalent scheme for estimating out-of-sample performance in both model selection and assessment. For time series data, however, it is unclear whether forwardvalidation schemes, i.e., schemes that keep the temporal order of observations, should be preferred. In this paper, we perform a comprehensive empirical study of eight common validation schemes. We introduce a study design that perturbs global stationarity by introducing a slow evolution of the underlying data-generating process. Our results demonstrate that, even for relatively small perturbations, commonly used cross-validation schemes often yield estimates with the largest bias and variance, and forward-validation schemes yield better estimates of the out-of-sample error. We provide an interpretation of these results in terms of an additional evolution-induced bias and the sample-size dependent estimation error. Using a large-scale financial data set, we demonstrate the practical significance in a replication study of a statistical arbitrage problem. We conclude with some general guidelines on the selection of suitable validation schemes for time series data.},
	language = {eng},
	number = {11/2019},
	urldate = {2022-04-14},
	institution = {FAU Discussion Papers in Economics},
	author = {Schnaubelt, Matthias},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Acer\\Zotero\\storage\\UBQIMXJC\\Schnaubelt - 2019 - A comparison of machine learning model validation .pdf:application/pdf;Snapshot:C\:\\Users\\Acer\\Zotero\\storage\\UTGYZIYU\\209136.html:text/html},
}
